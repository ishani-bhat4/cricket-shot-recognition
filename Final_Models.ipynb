{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Mount and Installs**"
      ],
      "metadata": {
        "id": "wfjopA_HzvX0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HneAcyozWQ5",
        "outputId": "c99a7bd0-5559-46d2-98aa-64abe757b51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install vit-keras\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "jbzVm-lO0J_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention Network**"
      ],
      "metadata": {
        "id": "sRmBlQfCzY6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#attention  networks model:\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.utils import resample\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Reshape, Attention, Bidirectional, LSTM, GlobalAveragePooling1D\n",
        "\n",
        "\n",
        "shot_types = ['sweep', 'drive', 'pull', 'slog', 'flick', 'cut']\n",
        "\n",
        "sample_fraction = 0.05\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    folder_path = f'/content/drive/MyDrive/Datasets/Dataset_1_40/{shot_type}'\n",
        "\n",
        "    # equal representation of each class\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_fraction, random_state=42)\n",
        "\n",
        "    for _, sampled_indices in sss.split(np.zeros(len(os.listdir(folder_path))), label * np.ones(len(os.listdir(folder_path)))):\n",
        "        sampled_files = [os.listdir(folder_path)[i] for i in sampled_indices]\n",
        "\n",
        "        for vid_file in sampled_files:\n",
        "            vid_path = os.path.join(folder_path, vid_file)\n",
        "            vid = cv2.VideoCapture(vid_path)\n",
        "\n",
        "            while True:\n",
        "                ret, frame = vid.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.resize(frame, (100, 100))\n",
        "                data.append(frame)\n",
        "                labels.append(label)\n",
        "\n",
        "            vid.release()\n",
        "\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def build_attention_model(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Reshape for time-distributed layer\n",
        "    x = Reshape((1, -1))(x)\n",
        "\n",
        "    # LSTM layer with attention mechanism\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    attention = Attention()([x, x])\n",
        "\n",
        "    # Global average pooling\n",
        "    x = GlobalAveragePooling1D()(attention)\n",
        "\n",
        "    # Dense layers\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "#input shape and number of classes\n",
        "input_shape = X_train[0].shape\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "attention_model = build_attention_model(input_shape, num_classes)\n",
        "\n",
        "attention_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "attention_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, batch_size=32)\n",
        "\n",
        "# model_save_path = '/content/drive/MyDrive/Datasets/Datasets/Shots/attention_model.h5'\n",
        "# attention_model.save(model_save_path)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = attention_model.evaluate(X_test, y_test)[1]\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "gIDmrZid1FLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d0e6c2-70dd-46ec-e4f4-8e6b2c49732c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "52/52 [==============================] - 11s 71ms/step - loss: 1.4335 - accuracy: 0.4400 - val_loss: 0.7817 - val_accuracy: 0.7567\n",
            "Epoch 2/4\n",
            "52/52 [==============================] - 2s 35ms/step - loss: 0.4124 - accuracy: 0.8631 - val_loss: 0.1624 - val_accuracy: 0.9465\n",
            "Epoch 3/4\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.0387 - accuracy: 0.9903 - val_loss: 0.0349 - val_accuracy: 0.9951\n",
            "Epoch 4/4\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 0.0074 - accuracy: 0.9994 - val_loss: 0.0270 - val_accuracy: 0.9951\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0270 - accuracy: 0.9951\n",
            "Test Accuracy: 0.9951338171958923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming attention_model.predict returns the predicted probabilities for each class\n",
        "y_pred_probabilities = attention_model.predict(X_test)\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "accuracy = attention_model.evaluate(X_test, y_test)[1]\n",
        "print('Test Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 Score:', f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6xgHKkgGWRz",
        "outputId": "2c5b6afc-8d1b-4438-e802-82eeb8c97205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 1s 12ms/step\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0270 - accuracy: 0.9951\n",
            "Test Accuracy: 0.9951338171958923\n",
            "Precision: 0.995183474849794\n",
            "Recall: 0.9951338199513382\n",
            "F1 Score: 0.9951276455753844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN+RNN**"
      ],
      "metadata": {
        "id": "DNUcec5Tzmbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Bidirectional, GlobalAveragePooling1D, Reshape\n",
        "\n",
        "shot_types = ['sweep', 'drive', 'pull', 'slog', 'flick', 'cut']\n",
        "\n",
        "sample_fraction = 0.8\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    # folder_path = f'/content/drive/MyDrive/Datasets/Datasets/Shots/{shot_type}'\n",
        "    folder_path = f'/content/drive/MyDrive/Dataset_1_40/{shot_type}'\n",
        "\n",
        "    # equal representation of each class\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_fraction, random_state=42)\n",
        "\n",
        "    for _, sampled_indices in sss.split(np.zeros(len(os.listdir(folder_path))), label * np.ones(len(os.listdir(folder_path)))):\n",
        "        sampled_files = [os.listdir(folder_path)[i] for i in sampled_indices]\n",
        "\n",
        "        for vid_file in sampled_files:\n",
        "            vid_path = os.path.join(folder_path, vid_file)\n",
        "            vid = cv2.VideoCapture(vid_path)\n",
        "\n",
        "            while True:\n",
        "                ret, frame = vid.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.resize(frame, (100, 100))\n",
        "                data.append(frame)\n",
        "                labels.append(label)\n",
        "\n",
        "            vid.release()\n",
        "\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "def build_cnn_rnn_model(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Reshape for time-distributed layer\n",
        "    x = Reshape((1, -1))(x)\n",
        "\n",
        "    # Bidirectional LSTM layer\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "\n",
        "    # Global average pooling\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Dense layers\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Input shape and number of classes\n",
        "input_shape = X_train[0].shape\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "cnn_rnn_model = build_cnn_rnn_model(input_shape, num_classes)\n",
        "\n",
        "cnn_rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "cnn_rnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n",
        "\n",
        "# model_save_path = '/content/drive/MyDrive/Shots/cnn_rnn_model.h5'\n",
        "# cnn_rnn_model.save(model_save_path)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = cnn_rnn_model.evaluate(X_test, y_test)[1]\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "IN5vgHEN0eAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming attention_model.predict returns the predicted probabilities for each class\n",
        "y_pred_probabilities = cnn_rnn_model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 Score:', f1)"
      ],
      "metadata": {
        "id": "4nA05zJiKW3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ViT Model 1**"
      ],
      "metadata": {
        "id": "2s3gA22szmho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from vit_keras import vit\n",
        "\n",
        "# Define shot types and other parameters\n",
        "#shot_types = ['drive', 'pull']\n",
        "shot_types=['sweep', 'drive', 'pull', 'slog', 'flick', 'cut']\n",
        "sample_fraction = 0.4\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    folder_path = f'/content/drive/MyDrive/Dataset_1_40/{shot_type}'\n",
        "\n",
        "    # Equal representation of each class\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_fraction, random_state=42)\n",
        "\n",
        "    for _, sampled_indices in sss.split(np.zeros(len(os.listdir(folder_path))), label * np.ones(len(os.listdir(folder_path)))):\n",
        "        sampled_files = [os.listdir(folder_path)[i] for i in sampled_indices]\n",
        "\n",
        "        for vid_file in sampled_files:\n",
        "            vid_path = os.path.join(folder_path, vid_file)\n",
        "            vid = cv2.VideoCapture(vid_path)\n",
        "\n",
        "            while True:\n",
        "                ret, frame = vid.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.resize(frame, (96, 96))  # Adjust the size based on the ViT model requirements\n",
        "                data.append(frame)\n",
        "                labels.append(label)\n",
        "\n",
        "            vid.release()\n",
        "\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load ViT model without classification head\n",
        "vit_model = vit.vit_l32(\n",
        "    image_size=(96, 96),\n",
        "    activation='softmax',\n",
        "    pretrained=True,\n",
        "    include_top=False,\n",
        "    pretrained_top=False,\n",
        "    classes=len(shot_types)\n",
        ")\n",
        "\n",
        "# Build the new model by adding a classification head\n",
        "input_layer = Input(shape=(96, 96, 3))\n",
        "x = vit_model(input_layer)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output_layer = Dense(len(shot_types), activation='softmax')(x)\n",
        "\n",
        "vit_model_with_head = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile and train the model\n",
        "vit_model_with_head.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "vit_model_with_head.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=6)\n",
        "\n",
        "# Save the model\n",
        "# model_save_path = '/content/drive/MyDrive/Shots/vit_model.h5'\n",
        "# vit_model_with_head.save(model_save_path)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = vit_model_with_head.evaluate(X_test, y_test)[1]\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "s8lrp_Y20YIw",
        "outputId": "7cd2efc0-bd27-4a2a-88f1-5041c56d2a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-L_32_imagenet21k+imagenet2012.npz\n",
            "1226658854/1226658854 [==============================] - 14s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 3, 3\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "616/616 [==============================] - 5733s 9s/step - loss: 1.6757 - accuracy: 0.2849 - val_loss: 1.6867 - val_accuracy: 0.2947\n",
            "Epoch 2/5\n",
            "333/616 [===============>..............] - ETA: 40:57 - loss: 1.6596 - accuracy: 0.2768"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b46a5bc392ae>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Compile and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mvit_model_with_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mvit_model_with_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ViT Model 2**"
      ],
      "metadata": {
        "id": "XA2DQmaUiyLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Bidirectional, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from vit_keras import vit\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define shot types and other parameters\n",
        "shot_types = ['sweep', 'drive', 'pull', 'slog', 'flick', 'cut']\n",
        "sample_fraction = 0.4\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    folder_path = f'/content/drive/MyDrive/Shots/{shot_type}'\n",
        "\n",
        "    # Equal representation of each class\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_fraction, random_state=42)\n",
        "\n",
        "    for _, sampled_indices in sss.split(np.zeros(len(os.listdir(folder_path))), label * np.ones(len(os.listdir(folder_path)))):\n",
        "        sampled_files = [os.listdir(folder_path)[i] for i in sampled_indices]\n",
        "\n",
        "        for vid_file in sampled_files:\n",
        "            vid_path = os.path.join(folder_path, vid_file)\n",
        "            vid = cv2.VideoCapture(vid_path)\n",
        "\n",
        "            while True:\n",
        "                ret, frame = vid.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.resize(frame, (96, 96))\n",
        "                data.append(frame)\n",
        "                labels.append(label)\n",
        "\n",
        "            vid.release()\n",
        "\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    # rotation_range=20,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # shear_range=0.2,\n",
        "    # zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load ViT model without classification head\n",
        "vit_model = vit.vit_l32(\n",
        "    image_size=(96, 96),\n",
        "    activation='softmax',\n",
        "    pretrained=True,\n",
        "    include_top=False,\n",
        "    pretrained_top=False,\n",
        "    classes=len(shot_types)\n",
        ")\n",
        "\n",
        "# Build the new model by adding a classification head\n",
        "input_layer = Input(shape=(96, 96, 3))\n",
        "x = vit_model(input_layer)\n",
        "x = Flatten()(x)\n",
        "\n",
        "# Reshape for time-distributed layer\n",
        "x = Reshape((1, -1))(x)\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "\n",
        "# Global average pooling\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "x = Dense(256, activation='relu')(x)\n",
        "output_layer = Dense(len(shot_types), activation='softmax')(x)\n",
        "\n",
        "vit_model_with_head = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile and train the model using the generator\n",
        "vit_model_with_head.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "vit_model_with_head.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                                  steps_per_epoch=len(X_train) // 32,\n",
        "                                  validation_data=(X_test, y_test),\n",
        "                                  epochs=4)\n",
        "\n",
        "# Save the model\n",
        "model_save_path = '/content/drive/MyDrive/Shots/vit_model_with_generator.h5'\n",
        "vit_model_with_head.save(model_save_path)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = vit_model_with_head.evaluate(X_test, y_test)[1]\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDnVhzz52wcQ",
        "outputId": "ced72482-1782-42d0-dc4d-2e45d65392b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-L_32_imagenet21k+imagenet2012.npz\n",
            "1226658854/1226658854 [==============================] - 7s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 3, 3\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "115/115 [==============================] - 148s 417ms/step - loss: 1.6650 - accuracy: 0.2787 - val_loss: 1.6821 - val_accuracy: 0.2557\n",
            "Epoch 2/4\n",
            "115/115 [==============================] - 39s 337ms/step - loss: 1.6551 - accuracy: 0.2790 - val_loss: 1.6780 - val_accuracy: 0.2557\n",
            "Epoch 3/4\n",
            "115/115 [==============================] - 45s 391ms/step - loss: 1.6511 - accuracy: 0.2863 - val_loss: 1.6795 - val_accuracy: 0.2947\n",
            "Epoch 4/4\n",
            "115/115 [==============================] - 42s 367ms/step - loss: 1.6524 - accuracy: 0.2721 - val_loss: 1.6766 - val_accuracy: 0.2947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 5s 162ms/step - loss: 1.6766 - accuracy: 0.2947\n",
            "Test Accuracy: 0.2946912348270416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ViT Model 3 - With Generator and Edits (Faster)**"
      ],
      "metadata": {
        "id": "RGJ0jzQgq5mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Bidirectional, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from vit_keras import vit\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define shot types and other parameters\n",
        "shot_types = ['sweep', 'drive', 'pull', 'slog', 'flick', 'cut']\n",
        "sample_fraction = 0.05\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    folder_path = f'/content/drive/MyDrive/Datasets/Dataset_1_40/{shot_type}'\n",
        "\n",
        "    # Equal representation of each class\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_fraction, random_state=42)\n",
        "\n",
        "    for _, sampled_indices in sss.split(np.zeros(len(os.listdir(folder_path))), label * np.ones(len(os.listdir(folder_path)))):\n",
        "        sampled_files = [os.listdir(folder_path)[i] for i in sampled_indices]\n",
        "\n",
        "        for vid_file in sampled_files:\n",
        "            vid_path = os.path.join(folder_path, vid_file)\n",
        "            vid = cv2.VideoCapture(vid_path)\n",
        "\n",
        "            while True:\n",
        "                ret, frame = vid.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.resize(frame, (96, 96))\n",
        "                data.append(frame)\n",
        "                labels.append(label)\n",
        "\n",
        "            vid.release()\n",
        "\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "#generator\n",
        "datagen = ImageDataGenerator(\n",
        "    # rotation_range=20,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # shear_range=0.2,\n",
        "    # zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "#ViT Model\n",
        "vit_model = vit.vit_l32(\n",
        "    image_size=(96, 96),\n",
        "    activation='softmax',\n",
        "    pretrained=True,\n",
        "    include_top=False,\n",
        "    pretrained_top=False,\n",
        "    classes=len(shot_types)\n",
        ")\n",
        "\n",
        "#classification head\n",
        "input_layer = Input(shape=(96, 96, 3))\n",
        "x = vit_model(input_layer)\n",
        "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Reshape((1, -1))(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "output_layer = Dense(len(shot_types), activation='softmax')(x)\n",
        "\n",
        "vit_model_with_head = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "#compile train\n",
        "vit_model_with_head.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "vit_model_with_head.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                                  steps_per_epoch=len(X_train) // 32,\n",
        "                                  validation_data=(X_test, y_test),\n",
        "                                  epochs=4)\n",
        "\n",
        "# Save\n",
        "# model_save_path = '/content/drive/MyDrive/Shots/vit_model_with_generator.h5'\n",
        "# vit_model_with_head.save(model_save_path)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = vit_model_with_head.evaluate(X_test, y_test)[1]\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1E3v_7XpAkk",
        "outputId": "a84217ec-c52a-49d5-b083-835e4609f6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "51/51 [==============================] - 8s 53ms/step - loss: 1.6705 - accuracy: 0.2713 - val_loss: 1.4266 - val_accuracy: 0.3990\n",
            "Epoch 2/4\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 1.0441 - accuracy: 0.5953 - val_loss: 0.6859 - val_accuracy: 0.7397\n",
            "Epoch 3/4\n",
            "51/51 [==============================] - 2s 31ms/step - loss: 0.3429 - accuracy: 0.8870 - val_loss: 0.1523 - val_accuracy: 0.9586\n",
            "Epoch 4/4\n",
            "51/51 [==============================] - 2s 34ms/step - loss: 0.0852 - accuracy: 0.9752 - val_loss: 0.0779 - val_accuracy: 0.9854\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0779 - accuracy: 0.9854\n",
            "Test Accuracy: 0.985401451587677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming attention_model.predict returns the predicted probabilities for each class\n",
        "y_pred_probabilities = vit_model_with_head.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 Score:', f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmdmWzsdOY5y",
        "outputId": "1a762588-9e61-4b46-ba6d-676da0d12d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 1s 9ms/step\n",
            "Precision: 0.9856642813238483\n",
            "Recall: 0.9854014598540146\n",
            "F1 Score: 0.9853736429410264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet50**"
      ],
      "metadata": {
        "id": "clq30OFApOxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import time\n",
        "data = []\n",
        "labels = []\n",
        "desired_frame_count = 5\n",
        "#, 'cut', 'slog', 'sweep', 'flick', 'misc'\n",
        "shot_types = ['drive','pull']\n",
        "\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    folder_path = f'/content/drive/MyDrive/Capstone 2023-24 SRU 02/Datasets/Shots/{shot_type}'\n",
        "    for vid_file in os.listdir(folder_path):\n",
        "        vid_path = os.path.join(folder_path, vid_file)\n",
        "        vid = cv2.VideoCapture(vid_path)\n",
        "        print('Processing video:', vid_file)\n",
        "        total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        middle_frame = total_frames // 2\n",
        "        frame_count = 0\n",
        "        while True:\n",
        "          ret, frame = vid.read()\n",
        "          if not ret:\n",
        "              break\n",
        "          frame_count +=1\n",
        "          if frame_count < middle_frame - desired_frame_count // 2:\n",
        "              continue\n",
        "          if frame_count > middle_frame + desired_frame_count // 2:\n",
        "              break\n",
        "\n",
        "          frame = cv2.resize(frame, (100, 100))\n",
        "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "          data.append(rgb_frame)\n",
        "          labels.append(label)\n",
        "        vid.release()\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.75, test_size=0.25, random_state=42)\n",
        "# ResNet model\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n",
        "\n",
        "# Freeze the layers of the ResNet model\n",
        "for layer in resnet.layers[:-2]:\n",
        "    layer.trainable = True\n",
        "for layer in resnet.layers[-2:]:\n",
        "    layer.trainable = False\n",
        "# Create a new model and add the ResNet base\n",
        "model = Sequential()\n",
        "model.add(resnet)\n",
        "\n",
        "# Add additional layers on top of the ResNet base\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(Dense(units=len(shot_types), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, batch_size=32)\n",
        "# Save\n",
        "model_save_path = '/content/drive/MyDrive/Datasets/Shots/resnet+cnn.h5'\n",
        "model.save(model_save_path)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = model.evaluate(X_test, y_test)[1]\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "WP-M79B1si5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9f65bb-a86e-497b-8b55-f34aa134bd66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 49s 219ms/step - loss: 2.7862 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.6540\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 0.5985 - accuracy: 0.6658 - val_loss: 0.6913 - val_accuracy: 0.6540\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.4629 - accuracy: 0.7878 - val_loss: 0.7084 - val_accuracy: 0.3460\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 3s 112ms/step - loss: 0.3649 - accuracy: 0.8628 - val_loss: 2.8812 - val_accuracy: 0.3460\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 0.3012 - accuracy: 0.8882 - val_loss: 0.8255 - val_accuracy: 0.3460\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 0.2228 - accuracy: 0.9174 - val_loss: 1.2035 - val_accuracy: 0.3460\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 3s 114ms/step - loss: 0.3041 - accuracy: 0.8767 - val_loss: 0.6479 - val_accuracy: 0.6540\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.2195 - accuracy: 0.9111 - val_loss: 0.7214 - val_accuracy: 0.3460\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.2216 - accuracy: 0.9174 - val_loss: 0.8098 - val_accuracy: 0.3460\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 3s 114ms/step - loss: 0.2008 - accuracy: 0.9072 - val_loss: 0.6501 - val_accuracy: 0.6540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 36ms/step - loss: 0.6501 - accuracy: 0.6540\n",
            "Test Accuracy: 0.6539924144744873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import time\n",
        "data = []\n",
        "labels = []\n",
        "desired_frame_count = 5\n",
        "#, 'cut', 'slog', 'sweep', 'flick', 'misc'\n",
        "shot_types = ['drive', 'pull', 'cut', 'slog', 'sweep', 'flick']\n",
        "\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    folder_path = f'/content/drive/MyDrive/Capstone 2023-24 SRU 02/Datasets/Shots/{shot_type}'\n",
        "    for vid_file in os.listdir(folder_path):\n",
        "        vid_path = os.path.join(folder_path, vid_file)\n",
        "        vid = cv2.VideoCapture(vid_path)\n",
        "        print('Processing video:', vid_file)\n",
        "        total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        middle_frame = total_frames // 2\n",
        "        frame_count = 0\n",
        "        while True:\n",
        "          ret, frame = vid.read()\n",
        "          if not ret:\n",
        "              break\n",
        "          frame_count +=1\n",
        "          if frame_count < middle_frame - desired_frame_count // 2:\n",
        "              continue\n",
        "          if frame_count > middle_frame + desired_frame_count // 2:\n",
        "              break\n",
        "\n",
        "          frame = cv2.resize(frame, (100, 100))\n",
        "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "          data.append(rgb_frame)\n",
        "          labels.append(label)\n",
        "        vid.release()\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.75, test_size=0.25, random_state=42)\n",
        "# ResNet model\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n",
        "\n",
        "# Freeze the layers of the ResNet model\n",
        "for layer in resnet.layers[:-2]:\n",
        "    layer.trainable = True\n",
        "for layer in resnet.layers[-2:]:\n",
        "    layer.trainable = False\n",
        "# Create a new model and add the ResNet base\n",
        "model = Sequential()\n",
        "model.add(resnet)\n",
        "\n",
        "# Add additional layers on top of the ResNet base\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(Dense(units=len(shot_types), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, batch_size=32)\n",
        "# Save\n",
        "model_save_path = '/content/drive/MyDrive/Datasets/Shots/resnet+cnn.h5'\n",
        "model.save(model_save_path)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = model.evaluate(X_test, y_test)[1]\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eeaf1a0-18f4-4122-fd3a-31b8e433ab2c",
        "id": "ZYJRyrNQsZMt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "53/53 [==============================] - 39s 168ms/step - loss: 3.4820 - accuracy: 0.3306 - val_loss: 1.6991 - val_accuracy: 0.3156\n",
            "Epoch 2/10\n",
            "53/53 [==============================] - 6s 116ms/step - loss: 1.4598 - accuracy: 0.3962 - val_loss: 1.7634 - val_accuracy: 0.2429\n",
            "Epoch 3/10\n",
            "53/53 [==============================] - 6s 112ms/step - loss: 1.2909 - accuracy: 0.4938 - val_loss: 1.7069 - val_accuracy: 0.1631\n",
            "Epoch 4/10\n",
            "53/53 [==============================] - 6s 117ms/step - loss: 1.0409 - accuracy: 0.5872 - val_loss: 1.7836 - val_accuracy: 0.1613\n",
            "Epoch 5/10\n",
            "53/53 [==============================] - 6s 118ms/step - loss: 0.8412 - accuracy: 0.6931 - val_loss: 1.8626 - val_accuracy: 0.1170\n",
            "Epoch 6/10\n",
            "53/53 [==============================] - 6s 114ms/step - loss: 0.6860 - accuracy: 0.7569 - val_loss: 1.8174 - val_accuracy: 0.1454\n",
            "Epoch 7/10\n",
            "53/53 [==============================] - 6s 119ms/step - loss: 0.5134 - accuracy: 0.8143 - val_loss: 4.1815 - val_accuracy: 0.1613\n",
            "Epoch 8/10\n",
            "53/53 [==============================] - 6s 120ms/step - loss: 0.4288 - accuracy: 0.8575 - val_loss: 3.0397 - val_accuracy: 0.1613\n",
            "Epoch 9/10\n",
            "53/53 [==============================] - 6s 118ms/step - loss: 0.3645 - accuracy: 0.8693 - val_loss: 2.0903 - val_accuracy: 0.1631\n",
            "Epoch 10/10\n",
            "53/53 [==============================] - 6s 118ms/step - loss: 0.2204 - accuracy: 0.9225 - val_loss: 2.0293 - val_accuracy: 0.1613\n",
            "18/18 [==============================] - 1s 38ms/step - loss: 2.0293 - accuracy: 0.1613\n",
            "Test Accuracy: 0.16134752333164215\n"
          ]
        }
      ]
    }
  ]
}